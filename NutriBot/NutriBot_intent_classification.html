<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Shilpa Musale</title>
        <link rel="icon" type="image/x-icon" href="../assets/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="../css/styles.css" rel="stylesheet" />
        <style>
            .indented {
              text-indent: 2em;
            }
            .intent-table {
    width: 100%;
    border-collapse: collapse;
    font-family: Arial, sans-serif;
    font-size: 14px;
    margin-top: 1em;
  }

  .intent-table th, .intent-table td {
    border: 1px solid #ddd;
    padding: 12px 16px;
    vertical-align: top;
  }

  .intent-table th {
    background-color: #f2f2f2;
    font-weight: bold;
    text-align: left;
    color: #333;
  }

  .intent-table tr:nth-child(even) {
    background-color: #fafafa;
  }

  .intent-table tr:hover {
    background-color: #f1f1f1;
  }

  .intent-table td:first-child {
    font-weight: 600;
    color: #2c3e50;
    white-space: nowrap;
  }
          </style>
    </head>
    
    <body>
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light" id="mainNav">
            <div class="container px-4 px-lg-5">
                <a class="navbar-brand" href="../index.html">Shilpa Musale (Ishi)</a>
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ms-auto py-4 py-lg-0">
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="../index.html">Home</a></li>
                        <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="../about.html">About</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        
        <!-- Page Header-->
        <header class="masthead" style="background-image: url('../assets/img/home-bg.jpg')">
            <div class="container position-relative px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <div class="post-heading">
                            <h1>Designing NutriBot: A Domain-Aware AI Assistant for Diet & Wellness</h1>
                            
                            <!-- <span class="meta">
                                Posted by
                                <a href="#!">Start Bootstrap</a>
                                on August 24, 2023
                            </span> -->
                        </div>
                    </div>
                </div>
            </div>
        </header>
        <!-- Post Content-->
        <div class="container px-4 px-lg-5">
            <div class="row gx-4 gx-lg-5">
                <!-- Left Sidebar Navigation -->
                <div class="col-md-3">
                    <nav id="toc" class="sticky-top">
                        <h3>Other Articles</h3>
                        <ul class="nav flex-column">
                            
                            <li class="nav-item"><a class="nav-link" href="../Recommendation_system/recommendation_intro.html">Personalizing Entertainment: The Math & Magic Behind Movie Suggestions</a></li>
                            <li class="nav-item"><a class="nav-link" href="../Sinai/sinai_intro.html">Leveraging Community Health Workers & Social Determinants of Health for Predicting Emergency Department Readmissions</a></li>
                            <li class="nav-item"><a class="nav-link" href="../Syndemics/syndemics_intro.html">Unlocking the Syndemics Approach: How Machine Learning Reveals Disease Interactions</a></li>
                        </ul>
                    </nav>
                </div>
                 <!-- Blog Post Content -->
                    <div class="col-md-9">
                        <article class="mb-4">
                            <div class="container" >
                                <!-- Section 1 -->
                                <!-- <h1 id="introduction" class="decorative-heading">Lights, Camera, Algorithms: <br/> ~ The Science of Movie Recommendations ~ </h1> -->
                                
                                <h2 class="decorative-heading">Section 3: Ms. Potts — Our LLM Agent + Intent Classifier</h2><br/>

                                <h3 class="decorative-heading">Orchestrating Thoughtful AI with Reasoning, Routing, and Tools</h3>
                                <p>Every query to NutriBot starts with a deceptively simple step: understanding what the user actually wants. Is it a request to log a meal? Ask for advice? Search a clinical document? This is where Ms. Potts comes in—our conversational brain that routes every question to the right backend action.</p>
                                
                                <h3 class="decorative-heading">Who is Ms. Potts?s</h3>
                                <p>Ms. Potts is our team’s implementation of a LangChain Agent, inspired by Disney’s internal agent framework of the same name. It’s not just a language model—it’s a decision-maker, built to understand the user’s intent and activate the right tools or data flows.
                                <p>It combines:</p>
                                <ul>

                                    <li>A foundation model (LLaMA 3.3, hosted on AWS Bedrock) that powers conversational understanding and reasoning</li><br/>

                                    <li>An intent classification layer (custom, under development) to route queries to the right tool or module</li><br/>
                                    
                                    <li>A registry of backend tools and APIs, each designed to serve a distinct user need (e.g., logging, planning, education)</li><br/>
                                    
                                    <li>A lightweight memory layer for multi-turn interaction (planned in the next phase)</li><br/>

                                </ul>
                                </p>
                                <h3 class="decorative-heading">Step 1: Understanding Intent classification (Classifier-in-the-loop)</h3>
                                <p>To ensure NutriBot responds appropriately to a variety of nutrition-related queries, the first step is to determine the user's intent—whether they're logging a meal, seeking personalized advice, or exploring general health topics.</p>
                                <p>Given the relatively bounded nature of the dietitian domain, we initially adopted a lightweight intent routing strategy using prompt-engineered reasoning within the LLM agent itself. This worked well for early prototyping, where routing logic could be inferred based on the structure of the prompt and tool descriptions.</p>
                                <p>However, as we began expanding the scope and fine-tuning the routing logic, we considered more robust and scalable approaches—based on our professor’s recommendations:</p>

                                <h5 class="decorative-heading">Future-Ready Intent Classification Strategies</h5><br/>
                                <table class="intent-table">
                                    <tr><th>Strategy</th><th>Description</th><th>AWS Bedrock Recommendation</th></tr>
                                    <tr>
                                        <td>Few-Shot Prompting</td>
                                        <td>Provide the LLM with examples of different intents (e.g., meal log, advice, education) in the prompt context. Suitable when intent types are limited and clearly distinguishable.</td>
                                        <td>✅ Use LLaMA 3.3 70B or Claude 3 Sonnet for larger context windows and strong in-context learning</td>
                                    </tr>
                                    <tr>
                                        <td>Fine-Tuning a Small LM</td>
                                        <td>Train a smaller model specifically on intent classification examples from the dietitian chatbot (e.g., using labeled user queries). Once trained, this can act as a fast, cost-efficient classifier.</td>
                                        <td>✅ Fine-tune Amazon Titan Text Lite or LLaMA 3.2 3B/7B using Amazon SageMaker or custom model endpoints</td>
                                    </tr>
                                    <tr>
                                        <td>Zero-shot + Embedding Similarity</td>
                                        <td>Convert user query to an embedding and compare against intent class exemplars. Low latency and interpretable, good for cold-start.</td>
                                        <td>✅ Use Amazon Titan Embeddings via Bedrock + a small classifier layer or cosine search</td>
                                    </tr>
                                </table>
                        <p>In our next phase, we're planning to:
                            <ul>
                            <li>Replace implicit prompt-based routing with a modular intent classifier</li><br/>

                            <li>Train a lightweight LLM (e.g., Titan Lite or LLaMA 3.2 3B) on labeled dietitian-domain queries</li><br/>

                            <li>Optionally fall back to few-shot prompting for new or long-tail intents</li><br/>

                            <li>This hybrid approach balances token efficiency, latency, and accuracy—and sets up the system to scale beyond nutrition, into other healthcare or wellness domains in the future.</li><br/>
                            </ul>

                        </p>

                        <h3 class="decorative-heading">Step 2: Tool Routing (The Agent at Work)</h3>                        
                        
                        <p>Once a user query is classified (explicitly or implicitly), Ms. Potts, our LLM agent, invokes one of several backend tools. These tools are defined as LangChain-compatible modules, each designed to perform a focused function in the nutrition domain.</p>
                        <p>From our current implementation, here’s what’s running under the hood:</p>
                        <table class="intent-table">
                            <tr><th>Tool</th><th>What It Does</th></tr>
                            <tr>
                                <td>log_meal()</td>
                                <td>Stores user-entered meals into the structured database, supporting features like meal tracking, calorie summaries, and progress history.</td>
                            </tr>
                            <tr>
                                <td>plan_meal()</td>
                                <td>Generates customized meal plans by blending user preferences, restrictions, and document-based nutritional guidelines retrieved via RAG.</td>
                            </tr>
                            <tr>
                                <td>education_facts()</td>
                                <td>Handles general nutrition questions like “Why is fiber important?” using RAG over educational corpora stored in ChromaDB.</td>
                            </tr>
                            <tr>
                                <td>personalized_advice()</td>
                                <td>Merges user-specific data (e.g., medical conditions, allergies) with retrieved knowledge to produce contextual advice, e.g., “What should I avoid with hypertension?”</td>
                            </tr>
                            <tr>
                                <td>image_classifier() (coming soon)</td>
                                <td>A vision tool that processes food images and estimates nutrition content. This will be powered by a food classification model and optionally routed to plan or log meals.</td>
                            </tr>
                            <tr>
                                <td>NutriBotQueryTool</td>
                                <td>Our custom retrieval-augmented generation tool, responsible for document-level question answering using ChromaDB for vector retrieval and LLaMA 3.3 via AWS Bedrock for response generation.</td>
                            </tr>
                            <tr>
                                <td>PubMedQueryTool</td>
                                <td>Connects to the PubMed API, pulling real biomedical literature and integrating it into the response pipeline when scientific evidence is needed.</td>
                            </tr>
                            <tr>
                                <td>WikipediaTool, DuckDuckGoSearchRun</td>
                                <td>External general-purpose tools used for lightweight fact checking and open-domain fallback—especially when queries fall outside of our structured or retrieved content.</td>
                            </tr>
                        </table>
                    <p>Each of these tools implements the LangChain BaseTool interface and defines a run() method. When a query is received, the agent uses a ZERO_SHOT_REACT_DESCRIPTION reasoning loop to decide which tool to call based on the tool descriptions.</p>

                    <p>Our most critical backend component is the NutriBotQueryTool, which enables retrieval-augmented generation (RAG) from our in-house nutrition corpus indexed in ChromaDB.</p>
                        
                    <p>In the future, we plan to add structured tools like log_meal() and plan_meal() to interact with the user profile database, and a vision module for image-based nutrition analysis.</p>
                        
                <h3  class="decorative-heading">Step 3: Action → Observation → Reason Loop</h3>
                <p>We implemented the ReAct-style Agent loop using LangChain’s AgentType.ZERO_SHOT_REACT_DESCRIPTION. In this design, the LLM first decides what to do (Thought), then acts (Action), observes the result (Observation), and generates a final answer.<br/>
                    Example trace:</p>
                    <ul>
                        <li>Thought: This question is about vitamin interactions. I should check the documents.</li>
                        <li>Action: NutriBotQueryTool → query = "What vitamins interact with iron?"</li>
                        <li>Observation: Retrieved 3 relevant documents from ChromaDB</li>
                        <li>Final Response: “Iron can interact with calcium and zinc supplements. It's best to separate intake by a few hours. Would you like meal suggestions to help manage this?”</li>
                    </ul>                   

                <p>This pattern allows the agent to simulate expert-level problem solving while maintaining modular control and transparency.</p>

                <h3  class="decorative-heading"> What We Learned</h3>
                <p>This architecture taught us the value of decoupling intelligence from monoliths. Instead of relying on a single LLM prompt to do everything, we built a modular, tool-aware reasoning engine that selects the right path dynamically.<br/>

                    Through this process, we learned to:</p>
                    <ul>
                    <li>Use LangChain’s agent framework for tool orchestration and decision-making</li><br/>

                    <li>Build reusable tools like NutriBotQueryTool that connect RAG pipelines to LLMs</li><br/>

                    <li>Leverage AWS Bedrock to deploy LLaMA 3.3 for reliable, scalable LLM inference</li><br/>

                    <li>Extend the agent with external APIs (e.g., PubMed) to enhance factual grounding</li><br/>

                    <li>Set a foundation for future features like structured meal tracking and multimodal reasoning</li>
                    </ul>

                    In short: We didn’t just build a chatbot—we engineered a modular reasoning engine that listens first, chooses smartly, and acts with purpose.


                                

                          
                                
                                
                            </div>
                            <div class="navigation">
                                <a href="./NutriBot_intent_classification.html" class="btn-nav" title="Go to the Section 3: Ms. Potts">Prev: Section 3-Ms. Potts</a>
                                <a href="./NutriBot_RAG_Engine.html" class="btn-nav" title="Go to RAG Engine"> Next: Section 4- Jarvis:Our RAG Engine</a>
                            </div> 
                        </article>
                    </div>
                </div>
            </div>
            
        
        <!-- Footer-->
        <footer class="border-top">
            <div class="container px-4 px-lg-5">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-md-10 col-lg-8 col-xl-7">
                        <ul class="list-inline text-center">
                            <li class="list-inline-item">
                                <a href="https://www.linkedin.com/in/shilpamusale/" target="_blank">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-linkedin-in fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            
                            <li class="list-inline-item">
                                <a href="https://github.com/ishi3012">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li class="list-inline-item">
                                <a href="mailto:ishishiv3012@gmail.com">
                                    <span class="fa-stack fa-lg">
                                        <i class="fas fa-circle fa-stack-2x"></i>
                                        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                        </ul>
                        <div class="small text-center text-muted fst-italic">Copyright &copy; The Code Diary 2025</div>
                    </div>
                </div>
            </div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
